{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bdfda-f997-4f46-a86f-94a9ecda33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (42000, 32, 32, 1) Test shape: (18000, 32, 32, 1)\n",
      "PCA-picked latent dimension to retain 95% energy: K = 163\n",
      "WARNING:tensorflow:From C:\\Users\\ashwini.sawant\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ae_single\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ae_single\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flat_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">167,075</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">167,936</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flat_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m)            │       \u001b[38;5;34m167,075\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m167,936\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">335,011</span> (1.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m335,011\u001b[0m (1.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">335,011</span> (1.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m335,011\u001b[0m (1.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "165/165 - 2s - 12ms/step - loss: 0.4357 - val_loss: 0.2774\n",
      "Epoch 2/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.2314 - val_loss: 0.2057\n",
      "Epoch 3/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.1795 - val_loss: 0.1672\n",
      "Epoch 4/25\n",
      "165/165 - 1s - 6ms/step - loss: 0.1516 - val_loss: 0.1451\n",
      "Epoch 5/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.1332 - val_loss: 0.1300\n",
      "Epoch 6/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.1201 - val_loss: 0.1193\n",
      "Epoch 7/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.1101 - val_loss: 0.1097\n",
      "Epoch 8/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.1023 - val_loss: 0.1026\n",
      "Epoch 9/25\n",
      "165/165 - 1s - 9ms/step - loss: 0.0962 - val_loss: 0.0970\n",
      "Epoch 10/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.0911 - val_loss: 0.0923\n",
      "Epoch 11/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0870 - val_loss: 0.0884\n",
      "Epoch 12/25\n",
      "165/165 - 1s - 9ms/step - loss: 0.0835 - val_loss: 0.0852\n",
      "Epoch 13/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0806 - val_loss: 0.0824\n",
      "Epoch 14/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0782 - val_loss: 0.0801\n",
      "Epoch 15/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.0760 - val_loss: 0.0781\n",
      "Epoch 16/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.0742 - val_loss: 0.0764\n",
      "Epoch 17/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0726 - val_loss: 0.0749\n",
      "Epoch 18/25\n",
      "165/165 - 1s - 9ms/step - loss: 0.0713 - val_loss: 0.0736\n",
      "Epoch 19/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.0701 - val_loss: 0.0725\n",
      "Epoch 20/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0692 - val_loss: 0.0715\n",
      "Epoch 21/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0683 - val_loss: 0.0706\n",
      "Epoch 22/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0675 - val_loss: 0.0698\n",
      "Epoch 23/25\n",
      "165/165 - 1s - 8ms/step - loss: 0.0668 - val_loss: 0.0690\n",
      "Epoch 24/25\n",
      "165/165 - 1s - 7ms/step - loss: 0.0661 - val_loss: 0.0684\n",
      "Epoch 25/25\n",
      "165/165 - 1s - 6ms/step - loss: 0.0654 - val_loss: 0.0677\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Single-layer AE test MSE: 0.0037704774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ae_conv\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ae_conv\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ img_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bottleneck (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">333,987</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">335,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ img_input (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bottleneck (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m)            │       \u001b[38;5;34m333,987\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │       \u001b[38;5;34m335,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m73,792\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m289\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,002,660</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,002,660\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,002,660</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,002,660\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "165/165 - 33s - 201ms/step - loss: 0.0457 - val_loss: 0.0169\n",
      "Epoch 2/20\n",
      "165/165 - 31s - 186ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 3/20\n",
      "165/165 - 31s - 187ms/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 4/20\n",
      "165/165 - 30s - 182ms/step - loss: 0.0083 - val_loss: 0.0078\n",
      "Epoch 5/20\n",
      "165/165 - 30s - 181ms/step - loss: 0.0073 - val_loss: 0.0076\n",
      "Epoch 6/20\n",
      "165/165 - 41s - 246ms/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 7/20\n",
      "165/165 - 30s - 181ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 8/20\n",
      "165/165 - 43s - 260ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 9/20\n",
      "165/165 - 32s - 192ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 10/20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "task3_autoencoders.py\n",
    "\n",
    "Task 3 only: design & train autoencoders on CIFAR-10 (grayscale).\n",
    "- Computes K using PCA (95% explained variance) on the 70% training subset.\n",
    "- Trains:\n",
    "    1) Single hidden-layer dense AE (sigmoid encoder, linear decoder) latent_dim=K\n",
    "    2) Deep convolutional AE with bottleneck size = K\n",
    "    3) 3-hidden-layer dense AE where total encoder hidden nodes sum approx K (distributed equally)\n",
    "- Reports reconstruction MSE on test set and saves models.\n",
    "\n",
    "Requirements:\n",
    "    - Python 3.8+\n",
    "    - numpy, scikit-learn, matplotlib\n",
    "    - tensorflow (>=2.4)  (Keras included)\n",
    "Run:\n",
    "    python task3_autoencoders.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- try imports for TF/Keras ---\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorFlow (tensorflow) is required to run this script. \"\n",
    "                       \"Please run on the course VM or install tensorflow. Error: \" + str(e))\n",
    "\n",
    "# ----------------------------\n",
    "# Config / hyperparameters\n",
    "# ----------------------------\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS_SINGLE = 25       # single-layer dense AE\n",
    "EPOCHS_CONV = 20         # conv AE\n",
    "EPOCHS_3LAYER = 30       # 3-layer dense AE\n",
    "\n",
    "MODEL_SAVE_DIR = \"./models\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions\n",
    "# ----------------------------\n",
    "def rgb2gray(images):\n",
    "    # images in [0,1], shape (N,H,W,3)\n",
    "    r, g, b = images[..., 0], images[..., 1], images[..., 2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray[..., np.newaxis]\n",
    "\n",
    "def recon_mse(orig, recon):\n",
    "    # mean MSE per image averaged\n",
    "    orig_flat = orig.reshape(orig.shape[0], -1)\n",
    "    recon_flat = recon.reshape(recon.shape[0], -1)\n",
    "    return np.mean(np.mean((orig_flat - recon_flat) ** 2, axis=1))\n",
    "\n",
    "# ----------------------------\n",
    "# Load CIFAR-10, convert to grayscale, split 70/30\n",
    "# ----------------------------\n",
    "(x_train0, y_train0), (x_test0, y_test0) = keras.datasets.cifar10.load_data()\n",
    "x_all = np.vstack([x_train0, x_test0]).astype(\"float32\") / 255.0\n",
    "y_all = np.vstack([y_train0, y_test0]).squeeze()\n",
    "\n",
    "x_gray = rgb2gray(x_all)  # shape (60000, 32, 32, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_gray, y_all, train_size=0.7, stratify=y_all, random_state=RANDOM_SEED\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "H, W = X_train.shape[1], X_train.shape[2]\n",
    "input_dim = H * W\n",
    "\n",
    "# Flattened standardized arrays for PCA & dense AEs\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Standardize (zero mean, unit var) using training statistics\n",
    "mean_train = X_train_flat.mean(axis=0)\n",
    "std_train = X_train_flat.std(axis=0) + 1e-9\n",
    "X_train_std = (X_train_flat - mean_train) / std_train\n",
    "X_test_std = (X_test_flat - mean_train) / std_train\n",
    "\n",
    "# ----------------------------\n",
    "# PCA to find K for 95% explained variance\n",
    "# ----------------------------\n",
    "pca = PCA(n_components=0.95, svd_solver='full', random_state=RANDOM_SEED)\n",
    "pca.fit(X_train_std)\n",
    "K = pca.n_components_\n",
    "print(f\"PCA-picked latent dimension to retain 95% energy: K = {K}\")\n",
    "\n",
    "latent_dim = int(K)\n",
    "\n",
    "# ----------------------------\n",
    "# Model 1: Single hidden-layer dense AE (sigmoid encoder, linear decoder)\n",
    "# ----------------------------\n",
    "tf.keras.backend.clear_session()\n",
    "inp = keras.Input(shape=(input_dim,), name=\"flat_input\")\n",
    "encoded = layers.Dense(latent_dim, activation='sigmoid', name=\"encoder_dense\")(inp)\n",
    "decoded = layers.Dense(input_dim, activation='linear', name=\"decoder_dense\")(encoded)\n",
    "ae_single = keras.Model(inp, decoded, name=\"ae_single\")\n",
    "ae_single.compile(optimizer='adam', loss='mse')\n",
    "ae_single.summary()\n",
    "\n",
    "# Train on standardized flattened data\n",
    "history_single = ae_single.fit(\n",
    "    X_train_std, X_train_std,\n",
    "    epochs=EPOCHS_SINGLE, batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test_std, X_test_std),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Reconstruct and invert standardization\n",
    "recon_test_std = ae_single.predict(X_test_std)\n",
    "recon_test_flat = recon_test_std * std_train + mean_train\n",
    "recon_single = recon_test_flat.reshape(-1, H, W, 1)\n",
    "mse_single = recon_mse(X_test, recon_single)\n",
    "print(\"Single-layer AE test MSE:\", mse_single)\n",
    "\n",
    "# ----------------------------\n",
    "# Model 2: Deep convolutional autoencoder with bottleneck of size K\n",
    "# ----------------------------\n",
    "tf.keras.backend.clear_session()\n",
    "input_img = keras.Input(shape=(H, W, 1), name=\"img_input\")\n",
    "x_enc = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(input_img)  # 16x16x32\n",
    "x_enc = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x_enc)      # 8x8x64\n",
    "x_enc = layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x_enc)     # 4x4x128\n",
    "shape_before_flat = tf.keras.backend.int_shape(x_enc)[1:]  # tuple (4,4,128)\n",
    "x_flat = layers.Flatten()(x_enc)\n",
    "bottleneck = layers.Dense(latent_dim, name=\"bottleneck\")(x_flat)  # linear bottleneck\n",
    "# decoder\n",
    "x_dec = layers.Dense(np.prod(shape_before_flat), activation='relu')(bottleneck)\n",
    "x_dec = layers.Reshape(shape_before_flat)(x_dec)\n",
    "x_dec = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x_dec)  # 8x8\n",
    "x_dec = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x_dec)   # 16x16\n",
    "x_dec = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x_dec)   # 32x32\n",
    "decoded_img = layers.Conv2D(1, 3, padding='same', activation='linear')(x_dec)\n",
    "\n",
    "ae_conv = keras.Model(input_img, decoded_img, name=\"ae_conv\")\n",
    "ae_conv.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n",
    "ae_conv.summary()\n",
    "\n",
    "history_conv = ae_conv.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=EPOCHS_CONV, batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, X_test),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "recon_conv = ae_conv.predict(X_test)\n",
    "mse_conv = recon_mse(X_test, recon_conv)\n",
    "print(\"Convolutional AE test MSE:\", mse_conv)\n",
    "\n",
    "# ----------------------------\n",
    "# Model 3: 3-hidden-layer dense AE where total hidden nodes ≈ K and are split equally\n",
    "# Interpretation: sum of encoder hidden nodes ≈ K and the last encoder layer acts as bottleneck.\n",
    "# ----------------------------\n",
    "tf.keras.backend.clear_session()\n",
    "nodes_each = int(math.ceil(latent_dim / 3.0))\n",
    "sizes = [nodes_each, nodes_each, max(1, latent_dim - 2 * nodes_each)]\n",
    "print(\"3-layer encoder sizes (sum approx K):\", sizes, \"-> bottleneck size:\", sizes[-1])\n",
    "\n",
    "inp3 = keras.Input(shape=(input_dim,), name=\"flat_input_3\")\n",
    "h1 = layers.Dense(sizes[0], activation='sigmoid')(inp3)\n",
    "h2 = layers.Dense(sizes[1], activation='sigmoid')(h1)\n",
    "bott3 = layers.Dense(sizes[2], activation='sigmoid', name='bottleneck3')(h2)\n",
    "# decoder (mirror)\n",
    "d1 = layers.Dense(sizes[1], activation='sigmoid')(bott3)\n",
    "d2 = layers.Dense(sizes[0], activation='sigmoid')(d1)\n",
    "out3 = layers.Dense(input_dim, activation='linear')(d2)\n",
    "\n",
    "ae_3layer = keras.Model(inp3, out3, name=\"ae_3layer\")\n",
    "ae_3layer.compile(optimizer='adam', loss='mse')\n",
    "ae_3layer.summary()\n",
    "\n",
    "history_3 = ae_3layer.fit(\n",
    "    X_train_std, X_train_std,\n",
    "    epochs=EPOCHS_3LAYER, batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test_std, X_test_std),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "recon_3_std = ae_3layer.predict(X_test_std)\n",
    "recon_3_flat = recon_3_std * std_train + mean_train\n",
    "recon_3 = recon_3_flat.reshape(-1, H, W, 1)\n",
    "mse_3layer = recon_mse(X_test, recon_3)\n",
    "print(\"3-hidden-layer AE test MSE:\", mse_3layer)\n",
    "\n",
    "# ----------------------------\n",
    "# Visualize a few reconstructions\n",
    "# ----------------------------\n",
    "n_show = 6\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(4, n_show, figsize=(n_show * 2, 8))\n",
    "for i in range(n_show):\n",
    "    axs[0, i].imshow(X_test[i].squeeze(), cmap='gray')\n",
    "    axs[0, i].axis('off'); axs[0, i].set_title(\"orig\")\n",
    "    axs[1, i].imshow(recon_single[i].squeeze(), cmap='gray')\n",
    "    axs[1, i].axis('off'); axs[1, i].set_title(\"single AE\")\n",
    "    axs[2, i].imshow(recon_conv[i].squeeze(), cmap='gray')\n",
    "    axs[2, i].axis('off'); axs[2, i].set_title(\"conv AE\")\n",
    "    axs[3, i].imshow(recon_3[i].squeeze(), cmap='gray')\n",
    "    axs[3, i].axis('off'); axs[3, i].set_title(\"3-layer AE\")\n",
    "plt.suptitle(\"Reconstructions (rows: orig, single dense AE, conv AE, 3-layer AE)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Results summary and save models\n",
    "# ----------------------------\n",
    "results = {\n",
    "    \"PCA_95_K\": latent_dim,\n",
    "    \"MSE_single_dense_AE\": float(mse_single),\n",
    "    \"MSE_conv_AE\": float(mse_conv),\n",
    "    \"MSE_3layer_AE\": float(mse_3layer)\n",
    "}\n",
    "print(\"Results summary:\", results)\n",
    "\n",
    "# Save models to MODEL_SAVE_DIR\n",
    "ae_single.save(os.path.join(MODEL_SAVE_DIR, \"ae_single.h5\"))\n",
    "ae_conv.save(os.path.join(MODEL_SAVE_DIR, \"ae_conv.h5\"))\n",
    "ae_3layer.save(os.path.join(MODEL_SAVE_DIR, \"ae_3layer.h5\"))\n",
    "\n",
    "# Also save training histories (optional)\n",
    "import json\n",
    "def history_to_dict(h):\n",
    "    return {k: [float(v) for v in vals] for k, vals in h.history.items()}\n",
    "\n",
    "with open(os.path.join(MODEL_SAVE_DIR, \"hist_single.json\"), \"w\") as f:\n",
    "    json.dump(history_to_dict(history_single), f)\n",
    "with open(os.path.join(MODEL_SAVE_DIR, \"hist_conv.json\"), \"w\") as f:\n",
    "    json.dump(history_to_dict(history_conv), f)\n",
    "with open(os.path.join(MODEL_SAVE_DIR, \"hist_3layer.json\"), \"w\") as f:\n",
    "    json.dump(history_to_dict(history_3), f)\n",
    "\n",
    "print(f\"Models & histories saved to {MODEL_SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c793d-915b-4541-b04f-7c1a37a23935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
